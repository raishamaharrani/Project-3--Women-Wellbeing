{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox1p\n",
    "import pickle\n",
    "from os import path\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "#from plotnine import ggplot, aes, geom_line, geom_point, facet_wrap, theme\n",
    "import plotly.graph_objects as go\n",
    "# Machine learning algorithms and model evaluation\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from skforecast.model_selection import backtesting_forecaster\n",
    "#from skforecast.model_selection import grid_search_forecaster\n",
    "#from skforecast.model_selection import bayesian_search_forecaster\n",
    "#from lightgbm import LGBMRegressor\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "Load the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three datasets\n",
    "average_hours = pd.read_csv('Resources/average-usual-weekly-hours-worked-women-15-years-and-older.csv')\n",
    "employment_ratio = pd.read_csv(\"Resources/female-employment-to-population-ratio.csv\")\n",
    "wage_gap = pd.read_csv(\"Resources/gender-wage-gap-oecd.csv\")\n",
    "percapita_labor = pd.read_csv(\"Resources/female-labor-force-participation-rates-by-national-per-capita-income.csv\")\n",
    "school_years = pd.read_csv(\"Resources/mean-years-of-schooling-female.csv\")\n",
    "maternity_leave = pd.read_csv(\"Resources/paid-leave-at-least-14-weeks-mothers.csv\")\n",
    "labor_sector = pd.read_csv(\"Resources/share-of-female-workers-by-sector.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Columns and Rows for each of the dataset \n",
    "print(f'Shape of average_hours DataFrame: {average_hours.shape}')\n",
    "print(f'Shape of employment_ratio DataFrame: {employment_ratio.shape}')\n",
    "print(f'Shape of wage_gap DataFrame: {wage_gap.shape}')\n",
    "print(f'Shape of percapita_labor DataFrame: {percapita_labor.shape}')\n",
    "print(f'Shape of school_years DataFrame: {school_years.shape}')\n",
    "print(f'Shape of maternity_leave DataFrame: {maternity_leave.shape}')\n",
    "print(f'Shape of labor_sector DataFrame: {labor_sector.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns to make it shorter\n",
    "average_hours = average_hours.rename(columns={'Average weekly hours worked (women, 15+) (OECD Labor Force Statistics (2017))': 'Avg_Hours_Worked'})\n",
    "employment_ratio = employment_ratio.rename(columns={'Employment to population ratio, 15+, female (%) (national estimate)': 'Emp_Pop_Ratio'})\n",
    "wage_gap = wage_gap.rename(columns={'Gender wage gap (OECD 2017)': 'Gender_Wage_Gap'})\n",
    "percapita_labor = percapita_labor.rename(columns={'Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate)': 'Labor_Force'})\n",
    "school_years = school_years.rename(columns={'Mean years of schooling (ISCED 1 or higher), population 25+ years, female': 'School_Years_Mean'})\n",
    "maternity_leave = maternity_leave.rename(columns={'Paid leave of at least 14 weeks available to mothers (1=yes; 0=no)': 'Paid_Leave'})\n",
    "labor_sector = labor_sector.rename(columns={'Female share of employment in agriculture (%)': 'Argiculture','Female share of employment in industry (%)': 'Industry','Female share of employment in services (%)': 'Services'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null Values from these two datasets \n",
    "average_hours.dropna(inplace=True)\n",
    "employment_ratio.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "merged_df= average_hours.merge(employment_ratio, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "merged_df= merged_df .merge(wage_gap, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "merged_df= merged_df .merge(percapita_labor, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "merged_df= merged_df .merge(school_years, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "merged_df= merged_df .merge(maternity_leave, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "merged_df= merged_df .merge(labor_sector, on=['Entity', 'Code', 'Year'], how='outer')\n",
    "\n",
    "# Check the result\n",
    "print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values \n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop colum \"Continent\"\n",
    "merged_df = merged_df.drop(\"Continent\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = merged_df.duplicated().sum()\n",
    "print(f\"Number of duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with less than <= 3 letters\n",
    "filtered_df = merged_df[merged_df['Code'].str.len()<=3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "print(filtered_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some basic EDA- View the dataset's distribution\n",
    "print(filtered_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Null Values using the KNNImputer. For KNNImputer can only work on numerical columns. \n",
    "First step is to define numerical and catrgorical columns and then train only on the numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and catrgorical columns \n",
    "numerical_cols = ['Year', 'Avg_Hours_Worked', 'Emp_Pop_Ratio', 'Gender_Wage_Gap', \n",
    "                  'Labor_Force', 'GDP per capita, PPP (constant 2017 international $)', \n",
    "                  'School_Years_Mean', 'Paid_Leave', 'Argiculture', 'Industry', \n",
    "                  'Services']\n",
    "\n",
    "categorical_cols = ['Entity', 'Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the numerical columns with n_neighbor =5\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Impute null values in numerical columns using KNNImputer\n",
    "imputer_num = KNNImputer(n_neighbors=5)\n",
    "imputed_num_data = imputer_num.fit_transform(filtered_df[numerical_cols])\n",
    "\n",
    "# One-Hot Encode categorical columns\n",
    "#ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "#encoded_array = ohe.fit_transform(filtered_df[categorical_cols])\n",
    "#encoded_df = pd.DataFrame(encoded_array.toarray(), columns=ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate imputed numerical columns with one-hot encoded categorical columns\n",
    "#df_encoded = pd.concat([pd.DataFrame(imputed_num_data, columns=numerical_cols), encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values before imputation\n",
    "print(\"Null values before imputation:\")\n",
    "print(filtered_df[numerical_cols].isnull().sum())\n",
    "\n",
    "#imputed_num_data = imputer_num.fit_transform(filtered_df[numerical_cols])\n",
    "\n",
    "# Check null values after the imputation \n",
    "imputed_num_df = pd.DataFrame(imputed_num_data, columns=numerical_cols)\n",
    "imputed_num_df.reset_index(inplace=True, drop=True)\n",
    "cat_df= filtered_df[categorical_cols]\n",
    "cat_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(cat_df.isnull().sum())\n",
    "print(\"Null values after imputation:\")\n",
    "print(imputed_num_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the KNNimputer was only used on the numerical columns, we have to use concate function to add the categorical columns to the imputed dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the imputed numerical columns with the original categorical columns\n",
    "# pd.DataFrame(imputed_num_data, columns=numerical_cols)\n",
    "imputed_df = pd.concat([imputed_num_df, cat_df], axis=1)\n",
    "#imputed_df[categorical_cols] = imputed_df[categorical_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the new datset\n",
    "imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values after concating the numerical and categorical columns\n",
    "print(imputed_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do We Need This?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Code to ISO Numeric Values\n",
    "\n",
    "# Update Country Code to the ISO numeric value\n",
    "country_code_dict = {\n",
    "\"AFG\":4,\"ALA\":248,\"ALB\":8,\"DZA\":12,\"ASM\":16,\"AND\":20,\"AGO\":24,\"AIA\":660,\"ATA\":10,\"ATG\":28,\"ARG\":32,\"ARM\":51,\n",
    "\"ABW\":533,\"AUS\":36,\"AUT\":40,\"AZE\":31,\"BHS\":44,\"BHR\":48,\"BGD\":50,\"BRB\":52,\"BLR\":112,\"BEL\":56,\"BLZ\":84,\"BEN\":204,\n",
    "\"BMU\":60,\"BTN\":64,\"BOL\":68,\"BIH\":70,\"BWA\":72,\"BVT\":74,\"BRA\":76,\"IOT\":86,\"BRN\":96,\"BGR\":100,\"BFA\":854,\"BDI\":108,\n",
    "\"KHM\":116,\"CMR\":120,\"CAN\":124,\"CPV\":132,\"CYM\":136,\"CAF\":140,\"TCD\":148,\"CHL\":152,\"CHN\":156,\"CXR\":162,\"CCK\":166,\n",
    "\"COL\":170,\"COM\":174,\"COG\":178,\"COD\":180,\"COK\":184,\"CRI\":188,\"CIV\":384,\"HRV\":191,\"CUB\":192,\"CYP\":196,\"CZE\":203,\n",
    "\"DNK\":208,\"DJI\":262,\"DMA\":212,\"DOM\":214,\"ECU\":218,\"EGY\":818,\"SLV\":222,\"GNQ\":226,\"ERI\":232,\"EST\":233,\"ETH\":231,\n",
    "\"FLK\":238,\"FRO\":234,\"FJI\":242,\"FIN\":246,\"FRA\":250,\"GUF\":254,\"PYF\":258,\"ATF\":260,\"GAB\":266,\"GMB\":270,\"GEO\":268,\n",
    "\"DEU\":276,\"GHA\":288,\"GIB\":292,\"GRC\":300,\"GRL\":304,\"GRD\":308,\"GLP\":312,\"GUM\":316,\"GTM\":320,\"GGY\":831,\"GIN\":324,\n",
    "\"GNB\":624,\"GUY\":328,\"HTI\":332,\"HMD\":334,\"VAT\":336,\"HND\":340,\"HKG\":344,\"HUN\":348,\"ISL\":352,\"IND\":356,\"IDN\":360,\n",
    "\"IRN\":364,\"IRQ\":368,\"IRL\":372,\"IMN\":833,\"ISR\":376,\"ITA\":380,\"JAM\":388,\"JPN\":392,\"JEY\":832,\"JOR\":400,\"KAZ\":398,\n",
    "\"KEN\":404,\"KIR\":296,\"PRK\":408,\"KOR\":410,\"KWT\":414,\"KGZ\":417,\"LAO\":418,\"LVA\":428,\"LBN\":422,\"LSO\":426,\"LBR\":430,\n",
    "\"LBY\":434,\"LIE\":438,\"LTU\":440,\"LUX\":442,\"MAC\":446,\"MKD\":807,\"MDG\":450,\"MWI\":454,\"MYS\":458,\"MDV\":462,\"MLI\":466,\n",
    "\"MLT\":470,\"MHL\":584,\"MTQ\":474,\"MRT\":478,\"MUS\":480,\"MYT\":175,\"MEX\":484,\"FSM\":583,\"MDA\":498,\"MCO\":492,\"MNG\":496,\n",
    "\"MNE\":499,\"MSR\":500,\"MAR\":504,\"MOZ\":508,\"MMR\":104,\"NAM\":516,\"NRU\":520,\"NPL\":524,\"NLD\":528,\"ANT\":530,\"NCL\":540,\n",
    "\"NZL\":554,\"NIC\":558,\"NER\":562,\"NGA\":566,\"NIU\":570,\"NFK\":574,\"MNP\":580,\"NOR\":578,\"OMN\":512,\"PAK\":586,\"PLW\":585,\n",
    "\"PSE\":275,\"PAN\":591,\"PNG\":598,\"PRY\":600,\"PER\":604,\"PHL\":608,\"PCN\":612,\"POL\":616,\"PRT\":620,\"PRI\":630,\"QAT\":634,\n",
    "\"REU\":638,\"ROU\":642,\"RUS\":643,\"RWA\":646,\"BLM\":652,\"SHN\":654,\"KNA\":659,\"LCA\":662,\"MAF\":663,\"SPM\":666,\"VCT\":670,\n",
    "\"WSM\":882,\"SMR\":674,\"STP\":678,\"SAU\":682,\"SEN\":686,\"SRB\":688,\"SYC\":690,\"SLE\":694,\"SGP\":702,\"SVK\":703,\"SVN\":705,\n",
    "\"SLB\":90,\"SOM\":706,\"ZAF\":710,\"SGS\":239,\"ESP\":724,\"LKA\":144,\"SDN\":736,\"SUR\":740,\"SJM\":744,\"SWZ\":748,\"SWE\":752,\n",
    "\"CHE\":756,\"SYR\":760,\"TWN\":158,\"TJK\":762,\"TZA\":834,\"THA\":764,\"TLS\":626,\"TGO\":768,\"TKL\":772,\"TON\":776,\"TTO\":780,\n",
    "\"TUN\":788,\"TUR\":792,\"TKM\":795,\"TCA\":796,\"TUV\":798,\"UGA\":800,\"UKR\":804,\"ARE\":784,\"GBR\":826,\"USA\":840,\"UMI\":581,\n",
    "\"URY\":858,\"UZB\":860,\"VUT\":548,\"VEN\":862,\"VNM\":704,\"VGB\":92,\"VIR\":850,\"WLF\":876,\"ESH\":732,\"YEM\":887,\"ZMB\":894,\n",
    "\"ZWE\":716,\"EMU\":1,\"XKX\":2,\"SSD\":3\n",
    "}\n",
    "\n",
    "imputed_df_code = imputed_df.replace({\"Code\": country_code_dict})\n",
    "\n",
    "imputed_df_code.head(50)\n",
    "\n",
    "# imputed_df_code.to_csv('imputed_data_with_country_codes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic EDA (Exploratory Data Analysis)\n",
    "Top 10 Countries for each of the features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Average Hours Worked  \n",
    "top_countries = imputed_df.groupby('Entity')['Avg_Hours_Worked'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Avg_Hours_Worked')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Avg_Hours_Worked'])\n",
    "plt.title(\"Top 10 Countries by Avg_Hours_Worked\")\n",
    "plt.xlabel(\"Avg_Hours_Worked\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Emp_Pop_Ratio  \n",
    "top_countries = imputed_df.groupby('Entity')['Emp_Pop_Ratio'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Emp_Pop_Ratio')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Emp_Pop_Ratio'])\n",
    "plt.title(\"Top 10 Countries by Emp_Pop_Ratio\")\n",
    "plt.xlabel(\"Emp_Pop_Ratio\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Gender_Wage_Gap\n",
    "top_countries = merged_df.groupby('Entity')['Gender_Wage_Gap'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Gender_Wage_Gap')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Gender_Wage_Gap'])\n",
    "plt.title(\"Top 10 Countries by Gender_Wage_Gap\")\n",
    "plt.xlabel(\"Gender_Wage_Gap\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Argiculture\n",
    "top_countries = merged_df.groupby('Entity')['Argiculture'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Argiculture')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Argiculture'])\n",
    "plt.title(\"Top 10 Countries Female share of employment in Agriculture\")\n",
    "plt.xlabel(\"Argiculture\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Industry\n",
    "top_countries = merged_df.groupby('Entity')['Industry'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Industry')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Industry'])\n",
    "plt.title(\"Top 10 Countries by Female share of employment in industry\")\n",
    "plt.xlabel(\"Industry(%)\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Services\n",
    "top_countries = merged_df.groupby('Entity')['Services'].mean().reset_index()\n",
    "top_countries = top_countries.nlargest(10, 'Services')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_countries['Entity'], top_countries['Services'])\n",
    "plt.title(\"Top 10 Countries  by Female share of employment in services\")\n",
    "plt.xlabel(\"Services(%)\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC EDA: Bottom 10 Countries for each of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the bottom 10 countries by average hours worked\n",
    "bottom_countries = merged_df.groupby('Entity')['Avg_Hours_Worked'].mean().reset_index()\n",
    "bottom_countries = bottom_countries.nsmallest(10, 'Avg_Hours_Worked')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(bottom_countries['Entity'], bottom_countries['Avg_Hours_Worked'])\n",
    "plt.title(\"Bottom 10 Countries by Average Hours Worked\")\n",
    "plt.xlabel(\"Average Hours Worked\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 countries by Services\n",
    "bottom_countries = merged_df.groupby('Entity')['Services'].mean().reset_index()\n",
    "bottom_countries = bottom_countries.nsmallest(10, 'Services')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(bottom_countries['Entity'], bottom_countries['Services'])\n",
    "plt.title(\"Bottom 10 Countries by Female in Services\")\n",
    "plt.xlabel(\"Services\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A PROFESSIONAL WELLBEING SCORE 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that will be used to create the professional wellbeing score\n",
    "columns_to_use = ['Avg_Hours_Worked', 'Emp_Pop_Ratio', 'Gender_Wage_Gap', \n",
    "                  'Labor_Force', 'GDP per capita, PPP (constant 2017 international $)', \n",
    "                  'School_Years_Mean', 'Paid_Leave', 'Argiculture', 'Industry', \n",
    "                  'Services']\n",
    "\n",
    "# Scale the columns using Min-Max Scaler to have values between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "imputed_df[columns_to_use] = scaler.fit_transform(imputed_df[columns_to_use])\n",
    "\n",
    "# Calculate the professional wellbeing score as the average of the scaled columns\n",
    "imputed_df['Professional_Wellbeing_Score'] = imputed_df[columns_to_use].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputed_df[columns_to_use] = scaler.fit_transform(imputed_df[columns_to_use])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = imputed_df[columns_to_use]\n",
    "y = imputed_df['Professional_Wellbeing_Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Development #1 \n",
    "Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the features and target\n",
    "# Replace with your feature columns columns_to_use = [...]  \n",
    "X = imputed_df[columns_to_use]\n",
    "y = imputed_df['Professional_Wellbeing_Score']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(10, 200, 10),\n",
    "    'max_depth': [None] + list(np.arange(1, 20, 2)),\n",
    "    'min_samples_split': np.arange(2, 10, 1),\n",
    "    'min_samples_leaf': np.arange(1, 10, 1),\n",
    "    'max_features': [1.0, 'sqrt', 'log2']  # Avoid using 'auto'\n",
    "}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of parameter settings sampled\n",
    "    cv=5,        # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1    # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Predict using the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate and display the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #2 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #3 Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #4 Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 251, 50),\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "    'max_depth': np.arange(3, 8),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11)\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=gb_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"MSE:\", mse_best)\n",
    "print(\"R2:\", r2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBRegressor model\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 251, 50),\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "    'max_depth': np.arange(3, 8),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11)\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=gb_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"MSE:\", mse_best)\n",
    "print(\"R2:\", r2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"R2:\", r2_score(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nRandom Forest Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"R2:\", r2_score(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nGradient Boosting Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_gb))\n",
    "print(\"R2:\", r2_score(y_test, y_pred_gb))\n",
    "\n",
    "print(\"\\nXGBoost Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_xgb))\n",
    "print(\"R2:\", r2_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model #6 Neural Networks with relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Define the input features (X) and the target variable (y)\n",
    "X = imputed_df[columns_to_use]\n",
    "y = imputed_df['Professional_Wellbeing_Score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer with a single neuron\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Flatten y_pred and y_test to 1-dimensional arrays\n",
    "y_pred = y_pred.flatten()\n",
    "y_test = y_test.values.flatten()\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professional Wellbeing App using Gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_professional_wellbeing_score(entity_name):\n",
    "    # Search for the entity name in the dataset\n",
    "    entity_row = imputed_df[imputed_df[\"Entity\"] == entity_name]\n",
    "    if entity_row.empty:\n",
    "        print(f\"Entity '{entity_name}' not found in dataset\")\n",
    "        return \"Entity not found\"\n",
    "    else:\n",
    "        # Return the professional wellbeing score for the entity\n",
    "        print(f\"Entity '{entity_name}' found in dataset\")\n",
    "        return entity_row[\"Professional_Wellbeing_Score\"].values[0]\n",
    "\n",
    "# Create the Gradio app\n",
    "gr_interface = gr.Interface(\n",
    "    fn=get_professional_wellbeing_score,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Professional Wellbeing Score Lookup\",\n",
    "    description=\"Enter an entity name to get its professional wellbeing score\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budiling the gradio app to work with LLM to provide explaination and suggestion for each country's professional wellbeing score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the API key from the environment variable\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set it in your .env file.\")\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {openai_api_key}',\n",
    "    'Content-Type': 'application/json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanations(entity_name):\n",
    "    entity_row = imputed_df[imputed_df[\"Entity\"] == entity_name]\n",
    "    \n",
    "    # Calculate the average values of the features across all entities, excluding 'Year'\n",
    "    average_values = imputed_df.select_dtypes(include=['number']).drop('Year', axis=1).mean()\n",
    " \n",
    "    \n",
    "    # Identify the features that are higher or lower for the entity compared to the average\n",
    "    explanations = []\n",
    "    for feature in entity_row.columns:\n",
    "        if feature != \"Entity\" and feature != \"Professional_Wellbeing_Score\" and feature != 'Year' and entity_row[feature].dtype.kind in 'bifc':\n",
    "            if entity_row[feature].values[0] > average_values[feature]:\n",
    "                explanations.append(f\"The entity has a higher value for {feature} compared to the average, which is associated with higher professional wellbeing scores.\")\n",
    "            elif entity_row[feature].values[0] < average_values[feature]:\n",
    "                explanations.append(f\"The entity has a lower value for {feature} compared to the average, which is associated with higher professional wellbeing scores.\")\n",
    "    \n",
    "    # Define the professional score as high or low on a scale of 0-1\n",
    "    wellbeing_score = entity_row[\"Professional_Wellbeing_Score\"].values[0]\n",
    "    if wellbeing_score >= 0.5:\n",
    "        wellbeing_score_label = \"high\"\n",
    "    else:\n",
    "        wellbeing_score_label = \"low\"\n",
    "    \n",
    "    # Generate a paragraph explaining the professional wellbeing score\n",
    "    explanation_paragraph = f\"The entity {entity_name} has a professional wellbeing score of {wellbeing_score:.2f}, which is considered {wellbeing_score_label}. \"\n",
    "    explanation_paragraph += \"The following factors contribute to this score: \"\n",
    "    explanation_paragraph += \", \".join(explanations)\n",
    "    \n",
    "    return explanation_paragraph\n",
    "\n",
    "def generate_suggestions(entity_name):\n",
    "    wellbeing_score = get_professional_wellbeing_score(entity_name)\n",
    "    explanations = generate_explanations(entity_name)\n",
    "    \n",
    "    # Create a prompt for the LLM model\n",
    "    prompt = f\"Generate suggestions to improve professional wellbeing of women in {entity_name} based on the following explanations: {', '.join(explanations)}\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        suggestions = result['choices'][0]['message']['content'].strip()\n",
    "        return suggestions\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        return f\"HTTP error occurred: {err}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio app\n",
    "gr_interface = gr.Interface(\n",
    "    fn=lambda entity_name: (\n",
    "        get_professional_wellbeing_score(entity_name),\n",
    "        generate_explanations(entity_name),\n",
    "        generate_suggestions(entity_name)\n",
    "    ),\n",
    "    inputs=\"text\",\n",
    "    outputs=[\"text\", \"text\", \"text\"],\n",
    "    title=\"Professional Wellbeing Score Lookup and Suggestions\",\n",
    "    description=\"Enter an entity name to get its professional wellbeing score, explanations, and suggestions for improvement\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "gr_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique entities with the highest professional wellbeing scores\n",
    "unique_entities = imputed_df.sort_values('Professional_Wellbeing_Score', ascending=False).drop_duplicates('Entity')[['Entity', 'Professional_Wellbeing_Score']].head(10)\n",
    "print(\"Top 20 unique entities with the highest professional wellbeing scores:\")\n",
    "print(unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique entities with the lowest professional wellbeing scores\n",
    "unique_entities = imputed_df.sort_values('Professional_Wellbeing_Score').drop_duplicates('Entity')[['Entity', 'Professional_Wellbeing_Score']].head(10)\n",
    "print(\"Top 20 unique entities with the lowest professional wellbeing scores:\")\n",
    "print(unique_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the highest and lowest entities with professional wellbeing scores\n",
    "highest_entities = imputed_df.nlargest(20, 'Professional_Wellbeing_Score')[['Entity', 'Professional_Wellbeing_Score']]\n",
    "lowest_entities = imputed_df.nsmallest(20, 'Professional_Wellbeing_Score')[['Entity', 'Professional_Wellbeing_Score']]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot the highest entities\n",
    "ax[0].bar(highest_entities['Entity'], highest_entities['Professional_Wellbeing_Score'])\n",
    "ax[0].set_title('Highest Entities with Professional Wellbeing Scores')\n",
    "ax[0].set_xlabel('Entity')\n",
    "ax[0].set_ylabel('Professional Wellbeing Score')\n",
    "ax[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Plot the lowest entities\n",
    "ax[1].bar(lowest_entities['Entity'], lowest_entities['Professional_Wellbeing_Score'])\n",
    "ax[1].set_title('Lowest Entities with Professional Wellbeing Scores')\n",
    "ax[1].set_xlabel('Entity')\n",
    "ax[1].set_ylabel('Professional Wellbeing Score')\n",
    "ax[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Layout so plots do not overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
